model_type: vllm
model_path: ''
tp_size: 1
model_torch_dtype: float16
device: 'cuda'

dataset_path: ''
output_path: ''

instruction: eval_res/correctness.txt
chat_model: True

max_new_tokens: 1000
max_context_length: 2048
prompt_subs:
  'qid': 'qid'
  'reference_text': 'text'
  'question': 'question'
  'model_answer': 'model_answer'
  'generator': 'generator'
  'retriever': 'retriever'
